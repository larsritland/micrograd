{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\lrx\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# brew install graphviz\n",
    "%pip install graphviz\n",
    "from graphviz import Digraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.engine import Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir}) #, node_attr={'rankdir': 'TB'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ data %.4f | grad %.4f }\" % (n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a very simple example\n",
    "x = Value(1.0)\n",
    "y = (x * 2 + 1).relu()\n",
    "y.backward()\n",
    "#draw_dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple 2D neuron\n",
    "import random\n",
    "from micrograd import nn\n",
    "\n",
    "random.seed(1337)\n",
    "n = nn.Neuron(2)\n",
    "x = [Value(1.0), Value(-2.0)]\n",
    "y = n(x)\n",
    "y.backward()\n",
    "\n",
    "#dot = draw_dot(y)\n",
    "#dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.nn import MLP\n",
    "n = MLP(3,[4,4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=0.17491833012255345, grad=0),\n",
       " Value(data=-1.9336279665121072, grad=0),\n",
       " Value(data=-0.42706587737738166, grad=0),\n",
       " Value(data=0.15975564404725467, grad=0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0,-1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0,1.0],\n",
    "    [1.0, 1.0,-1.0]\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=2.5866850283970813, grad=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dot.render('gout')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training av very simple model: y = wx + b\n",
    "Using micrograd MLP and a dataset of 2 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9\n",
      "2.1\n",
      "MLP of [Layer of [LinearNeuron(1)]]\n"
     ]
    }
   ],
   "source": [
    "# Instantiating a 1-layer MLP and setting its parameters\n",
    "n = MLP(1,[1])   # 1 input, 1 output, y = wx + b\n",
    "params = n.parameters()\n",
    "params[0].data = 1.9\n",
    "params[1].data = 2.1\n",
    "for p in params:\n",
    "    print(p.data)\n",
    "print(n)\n",
    "\n",
    "# Loading the dataset consisting 2 samples\n",
    "xs = [[2.0],\n",
    "      [3.0]]\n",
    "ys = [6.0,8.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.9, 7.799999999999999]\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions\n",
    "ypred = [n(x) for x in xs]\n",
    "print([yp.data for yp in ypred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=0.05000000000000035, grad=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the loss\n",
    "loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.916\n",
      "2.1060000000000003\n"
     ]
    }
   ],
   "source": [
    "n.zero_grad()\n",
    "loss.backward()\n",
    "learning_rate = 0.01\n",
    "for p in n.parameters():\n",
    "    p.data -= p.grad * learning_rate\n",
    "    print(p.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw_dot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(data=0.025160000000000158, grad=1) 1.9272399999999998 2.1101600000000005\n",
      "Value(data=7.656090196463862e-05, grad=1) 1.9878236470835418 2.0314954381628723\n",
      "Value(data=5.247050521673717e-06, grad=1) 1.9968123448689865 2.008245210676154\n",
      "Value(data=3.5960311948400426e-07, grad=1) 1.9991655017471992 2.0021585189176463\n",
      "Value(data=2.46451607446296e-08, grad=1) 1.999781536174616 2.0005650800326187\n",
      "Value(data=1.6890397086579587e-09, grad=1) 1.9999428082169834 2.0001479326591287\n",
      "Value(data=1.1575721361195228e-10, grad=1) 1.9999850277269529 2.0000387273843936\n",
      "Value(data=7.933343684492234e-12, grad=1) 1.9999960803991699 2.0000101384664535\n",
      "Value(data=5.437064364293277e-13, grad=1) 1.9999989738852193 2.000002654155546\n",
      "Value(data=3.726255911042191e-14, grad=1) 1.9999997313727624 2.00000069483306\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    ypred = [n(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "    n.zero_grad()\n",
    "    loss.backward()\n",
    "    for p in n.parameters():\n",
    "        p.data -= p.grad * learning_rate\n",
    "    if i % 1000 == 0:\n",
    "        print(loss, params[0].data, params[1].data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training av very simple model: y = wx + b\n",
    "Using micrograd and a dataset of 2 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "x = [[Value(2.0)],\n",
    "     [Value(3.0)]]\n",
    "y = [Value(6.0),\n",
    "     Value(8.0)]\n",
    "\n",
    "# Weights initialization\n",
    "w = [Value(1.9)]\n",
    "b = Value(2.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=5.9, grad=0), Value(data=7.799999999999999, grad=0)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ypred = [sum((wi*xi for wi,xi in zip(w, xt)), b) for xt in x]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.05000000000000035 | grad: 0\n"
     ]
    }
   ],
   "source": [
    "# Calculating the loss\n",
    "loss = sum((yout - ygt)**2 for ygt, yout in zip(y, ypred))\n",
    "print(f'loss: {loss.data} | grad: {loss.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:[1.916], b:2.1060000000000003\n"
     ]
    }
   ],
   "source": [
    "# Reset gradients\n",
    "for wi in w:\n",
    "    wi.grad = 0\n",
    "b.grad = 0\n",
    "\n",
    "# Backpropagation - calculating gradients\n",
    "loss.backward()\n",
    "\n",
    "# Updating weights\n",
    "learning_rate = 0.01\n",
    "for wi in w:\n",
    "    wi.data -= wi.grad * learning_rate\n",
    "b.data -= b.grad * learning_rate\n",
    "print(f'w:{[wi.data for wi in w]}, b:{b.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.506518044405186e-08 w:[1.9997796824343979], b:2.0005698749298104\n",
      "loss: 1.917174553094562e-08 w:[1.99980731649646], b:2.000498396474904\n",
      "loss: 1.4664000824692564e-08 w:[1.9998314844645497], b:2.0004358834424942\n",
      "loss: 1.1216136779807323e-08 w:[1.999852621085011], b:2.0003812113147013\n",
      "loss: 8.578949617396645e-09 w:[1.9998711065746826], b:2.000333396620033\n",
      "loss: 6.5618294412276236e-09 w:[1.9998872734604456], b:2.0002915792421763\n",
      "loss: 5.0189833877748825e-09 w:[1.999901412560892], b:2.0002550069477585\n",
      "loss: 3.838898050240777e-09 w:[1.999913778217727], b:2.00022302185478\n",
      "loss: 2.9362795412408877e-09 w:[1.9999245928710025], b:2.0001950485982682\n",
      "loss: 2.2458886461229267e-09 w:[1.9999340510604886], b:2.0001705839803217\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    ypred = [sum((wi*xi for wi,xi in zip(w, xt)), b) for xt in x]\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(y, ypred))\n",
    "    for wi in w:\n",
    "        wi.grad = 0\n",
    "    b.grad = 0\n",
    "    loss.backward()\n",
    "    for wi in w:\n",
    "        wi.data -= wi.grad * learning_rate\n",
    "    b.data -= b.grad * learning_rate\n",
    "    if i % 100 == 0:\n",
    "        print(f'loss: {loss.data} w:{[wi.data for wi in w]}, b:{b.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
